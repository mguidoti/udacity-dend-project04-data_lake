# Project 4: Data Lake

## Summary
In this project, we help a fictional streaming startup called Sparkify to build an ETL pipeline that extracts data from [Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html), processes it using Spark, and loads back to a different S3 bucket into one fact and four dimensional tables. To complete this, we need to use a [Amazon EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html) in AWS to run the Spark script and conclude the ETL process.

### Data
- **Song Dataset**, which is a subset of the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/), and each file is a JSON with metadata regarding a song, partitioned by the first three letters of each song's track ID.
- **Log Dataset**, whch is the fictional streaming app activity log files in JSON format generated by an event simulator with the input of the **song dataset** previously mentioned. These files are partitioned by year and month.

### Database Schema
- `fact` **songplays**, records in log data associated with song plays i.e. records with page `NextSong`. Columns: `songplay_id`, `start_time`, `user_id`, `level`, `song_id`, `artist_id`, `session_id`, `location`, `user_agent`
- `dim` **users**, users in the app. Columns: `user_id`, `first_name`, `last_name`, `gender`, `level`
- `dim` **songs**, songs in music database. Columns: `song_id`, `title`, `artist_id`, `year`, `duration`
- `dim` **artists**, artists in music database. Columns: `artist_id`, `name`, `location`, `lattitude`, `longitude`
- `dim` **time**, timestamps of records in songplays broken down into specific units. Columns: `start_time`, `hour`, `day`, `week`, `month`, `year`, `weekday`

## Project Files
```
dl.cfg ..................... must contain AWS credentials, but empty in the repo
etl.py ..................... contains ETL Spark script
README.md
```

## How to Run
To run locally, clone this repo into a local folder. Then, make sure you have **Python** and **Spark** installed, as well as uncomment lines 151 and 152 of the `etl.py` file, and comment lines 153 and 154. Finally, open a terminal, navigate to the folder containing the files from this repository, and run `python etl.py`.

To run on AWS, make sure you instantiate a S3 bucket and an EMR cluster with Spark, and place the bucket on the variable `output` on line 154 of the `etl.py` file. Move the files from this repo to the cluster, and, open a terminal on it to run the ETL script. 

## Contribute
This is a course project and there is no intention to continue the development. If you have any suggestion to make, however, please, contact me.